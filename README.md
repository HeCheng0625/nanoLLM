# nanoLLM ðŸ”¬

A hands-on research & engineering playground for **learning modern LLM architectures** by **re-implementing**, **ablating**, and **benchmarking** key architectural and optimization variants under **fixed compute (FLOPs)**.

> **Base model**: this project uses the Qwen3 0.6B as baseline model, with training/eval tooling built on **ðŸ¤— Transformers** + **Accelerate** (and optional FSDP).

---

## ðŸ§© Features & Roadmap

### Features

* **Base Architecture:** RMSNorm, SwiGLU, RoPE, Multi-head Attention (MHA), Grouped Query Attention (GQA).

* **Attention**

  * MHA (baseline)
  * MLA (multi-head latent attention)
  * Gated attention (attention output gating / QKV gating variants)
  * Linear attention
  * Sparse attention
  * Sliding-window attention
  * Hybrid architecture

* **MoE**

  * Top-k routing
  * Load balancing losses

* **mHC / Hyper-Connections**

  * multi-stream / hyper residual pathways

* **Engram-style memory**

  * Engram: retrieval / hash memory modules

* **Optimization & Training**

  * AdamW baseline vs. **Muon**


### Roadmap

#### Phase 0 â€” Foundations

* [ ] Config system (YAML/OmegaConf or dataclasses)
* [ ] Dataset pipeline (streaming + shuffling + packing)
* [ ] Tokenizer integration via ðŸ¤— Transformers (reuse Qwen tokenizer)
* [ ] Baseline decoder-only model (Qwen3-style)
* [ ] Training loop with Accelerate (fp16/bf16, grad accumulation, ckpt)
* [ ] Evaluation harness (perplexity + small task suite)
* [ ] Logging (W&B or TensorBoard) + run manifest export

#### Phase 1 â€” Baseline + MoE (your stated priority)

* [ ] Dense baseline reproduction: stable loss curve, expected PPL
* [ ] MoE FFN block (Top-2/Top-1 routing)
* [ ] Load-balancing losses (aux loss variants) + metrics (expert usage entropy, overflow)
* [ ] Capacity factor + token dropping policy
* [ ] Fixed-FLOPs comparison scripts (dense vs MoE at matched compute)

#### Phase 2 â€” Attention variants

* [ ] Sliding-window attention
* [ ] Sparse attention (block/global-local)
* [ ] MLA-style KV compression family
* [ ] Gated attention variants
* [ ] Linear attention baseline (1â€“2 representative forms)
* [ ] Hybrid configs (e.g., every N layers global attention)

#### Phase 3 â€” Memory + residual/path tricks

* [ ] Engram memory module (retrieval + gating)
* [ ] mHC / hyper connections (n-stream residual)
* [ ] Combined ablations (MoE + attention + memory)

---

## Repository layout

```text
nanoLLM/
â”œâ”€â”€ configs/                        # YAML configs (model/train/eval)
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â”œâ”€â”€ baseline_0_6B.yaml      # Dense baseline (Qwen3-style)
â”‚   â”‚   â””â”€â”€ moe_a0_6B.yaml          # MoE baseline (priority)
â”‚   â”‚   â””â”€â”€ experimental_mhc.yaml
â”‚   â””â”€â”€ train/                      # Training hyperparameters
â”‚       â”œâ”€â”€ pretrain_adamw.yaml
â”‚       â””â”€â”€ pretrain_muon.yaml
â”‚
â”œâ”€â”€ src/                            # Python package root (HF-compatible)
â”‚   â””â”€â”€ nanollm/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ configuration_nanollm.py    # HF Config (PretrainedConfig)
â”‚       â”œâ”€â”€ modeling_nanollm.py         # HF Model (PreTrainedModel)
â”‚       â”œâ”€â”€ modeling_blocks.py          # TransformerBlock / MoEBlock assembly
â”‚       â”‚
â”‚       â”œâ”€â”€ components/                 # Pluggable building blocks
â”‚       â”‚   â”œâ”€â”€ norms.py                # RMSNorm / LayerNorm variants
â”‚       â”‚   â”œâ”€â”€ rotary.py               # RoPE utilities
â”‚       â”‚   â”œâ”€â”€ mlp.py                  # FFN / SwiGLU / gated variants
â”‚       â”‚   â”œâ”€â”€ attention/
â”‚       â”‚   â”‚   â”œâ”€â”€ mha.py
â”‚       â”‚   â”‚   â”œâ”€â”€ mla.py
â”‚       â”‚   â”‚   â”œâ”€â”€ linear.py
â”‚       â”‚   â”‚   â”œâ”€â”€ sparse.py
â”‚       â”‚   â”‚   â”œâ”€â”€ sliding_window.py
â”‚       â”‚   â”‚   â””â”€â”€ hybrid.py
â”‚       â”‚   â”œâ”€â”€ moe/
â”‚       â”‚   â”‚   â”œâ”€â”€ router.py           # top-k routing
â”‚       â”‚   â”‚   â”œâ”€â”€ experts.py          # expert MLP
â”‚       â”‚   â”‚   â””â”€â”€ losses.py           # load-balancing losses (DeepSeek-style)
â”‚       â”‚   â”œâ”€â”€ memory/
â”‚       â”‚   â”‚   â””â”€â”€ engram.py           # retrieval/hash memory + gating
â”‚       â”‚   â””â”€â”€ residual/
â”‚       â”‚       â””â”€â”€ mhc.py              # hyper-connections / multi-stream residuals
â”‚       â”‚
â”‚       â”œâ”€â”€ data/
â”‚       â”‚   â”œâ”€â”€ datasets.py             # HF datasets / streaming loaders
â”‚       â”‚   â”œâ”€â”€ packing.py              # sequence packing
â”‚       â”‚   â””â”€â”€ collate.py              # batch collation
â”‚       â”‚
â”‚       â”œâ”€â”€ optim/                      # One true place for optim & schedulers
â”‚       â”‚   â”œâ”€â”€ adamw.py
â”‚       â”‚   â”œâ”€â”€ muon.py                 # Custom Muon optimizer
â”‚       â”‚   â””â”€â”€ schedulers.py           # cosine/warmup/etc.
â”‚       â”‚
â”‚       â”œâ”€â”€ train/
â”‚       â”‚   â”œâ”€â”€ trainer.py              # accelerate-based trainer
â”‚       â”‚   â”œâ”€â”€ losses.py               # LM loss + aux losses (moe, engram, mtp)
â”‚       â”‚   â””â”€â”€ hooks.py                # optional: callbacks (log, ckpt, eval)
â”‚       â”‚
â”‚       â”œâ”€â”€ eval/
â”‚       â”‚   â”œâ”€â”€ perplexity.py
â”‚       â”‚   â”œâ”€â”€ harness.py              # downstream eval glue
â”‚       â”‚   â””â”€â”€ metrics.py
â”‚       â”‚
â”‚       â””â”€â”€ utils/
â”‚           â”œâ”€â”€ flops.py                # CRITICAL: FLOPs estimator/accounting
â”‚           â”œâ”€â”€ metrics.py              # training metrics + EMA, etc.
â”‚           â”œâ”€â”€ logging.py              # W&B/TB logger adapters
â”‚           â”œâ”€â”€ checkpoint.py
â”‚           â””â”€â”€ seed.py
â”‚
â”œâ”€â”€ scripts/                           # CLI entrypoints
â”‚   â”œâ”€â”€ train.py
â”‚   â”œâ”€â”€ eval.py
â”‚   â”œâ”€â”€ estimate_flops.py
â”‚   â””â”€â”€ sweep.py                        # optional
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_shapes.py
â”‚   â”œâ”€â”€ test_attention_equivalence.py
â”‚   â””â”€â”€ test_moe_routing.py
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ pyproject.toml                      # (recommended) or setup.cfg
â””â”€â”€ README.md
```

---

## Quickstart

### 1. Train dense baseline

```bash
python scripts/train.py \
  --config configs/model/baseline_0_6B.yaml \
  --train-config configs/train/pretrain_adamw.yaml
```

### 2. Train MoE variant

```bash
python scripts/train.py \
  --config configs/model/moe_a0_6B.yaml \
  --train-config configs/train/pretrain_muon.yaml
```

### 3. Evaluate perplexity

```bash
python scripts/eval.py \
  --config configs/model/baseline_0_6B.yaml \
  --ckpt path/to/checkpoint
```
### 4. Estimate FLOPs

```bash
python scripts/estimate_flops.py \
  --config configs/model/baseline_0_6B.yaml
```

---

## Implementation notes (what to reuse vs write)

Reuse from ðŸ¤— Transformers:

* tokenizer / vocab / special tokens
* dataset loading utilities
* (optionally) weight init conventions / config patterns

Write in nanoLLM:

* a clean, minimal Qwen3-style model (so you understand it)
* attention / MoE / memory variants as components
* fixed-FLOPs accounting + fair benchmarking harness

---

## TODO (starter list)

### Baseline model

* [ ] RMSNorm + RoPE + SwiGLU FFN
* [ ] KV cache support + causal mask correctness tests
* [ ] HF-compatible `Config` + `from_pretrained`-style loading (optional)

### MoE (priority)

* [ ] Router: top-1/top-2, jitter noise, z-loss (optional)
* [ ] Losses: balance/importance/load losses (DeepSeek-like variants)
* [ ] Capacity factor + dispatch implementation
* [ ] Metrics dashboard for expert load

### Attention variants

* [ ] Sliding window attention with KV cache
* [ ] Block sparse attention
* [ ] MLA module + ablation knobs (rank, shared projection, etc.)
* [ ] Gated attention (output gate, Q/K gate variants)
* [ ] Linear attention baseline

### Engram + mHC

* [ ] Retrieval table + hashing + gating API
* [ ] Plug memory into attention context or FFN residual
* [ ] mHC multi-stream residual with fused-friendly layout (later)
